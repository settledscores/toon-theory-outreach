import os
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from dotenv import load_dotenv
from airtable import Airtable
from playwright.sync_api import sync_playwright

load_dotenv()

AIRTABLE_API_KEY = os.getenv("AIRTABLE_API_KEY")
AIRTABLE_BASE_ID = os.getenv("AIRTABLE_BASE_ID")
SCRAPER_TABLE_NAME = os.getenv("SCRAPER_TABLE_NAME")

airtable = Airtable(AIRTABLE_BASE_ID, SCRAPER_TABLE_NAME, AIRTABLE_API_KEY)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; Bot/0.1; +https://example.com/bot)"
}

BASE_URL = "https://www.bbb.org"
SEARCH_URL = "https://www.bbb.org/search?find_country=USA&find_entity=60005-101&find_id=1_100&find_latlng=30.314613%2C-97.793745&find_loc=Austin%2C%20TX&find_text=Accounting&find_type=Category&page=1&sort=Relevance"

MAX_LEADS = 5
WAIT_BETWEEN = 3  # seconds


def get_profile_links():
    print("ğŸ§­ Launching Playwright to fetch BBB search results...")
    links = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context()
        page = context.new_page()
        page.goto(SEARCH_URL, timeout=60000)
        page.wait_for_selector("a.Link__StyledLink-sc-1dh2vhu-0", timeout=20000)

        elements = page.query_selector_all("a.Link__StyledLink-sc-1dh2vhu-0")
        for el in elements:
            href = el.get_attribute("href")
            if href and "/profile/" in href:
                full_url = urljoin(BASE_URL, href)
                if full_url not in links:
                    links.append(full_url)

        browser.close()

    print(f"ğŸ”— Extracted {len(links)} profile links.")
    return links[:MAX_LEADS]


def scrape_profile(url):
    print(f"â¡ï¸ Scraping profile: {url}")
    try:
        res = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(res.text, 'html.parser')

        website_url = soup.find("a", string="Visit Website")
        notes = soup.select_one("section p")
        location = soup.select_one("address")
        years = soup.find(text=lambda t: t and "years in business" in t.lower())

        decision_section = soup.find("h2", string=lambda s: s and "Business Management" in s)
        name, title = "", ""
        if decision_section:
            li = decision_section.find_next("li")
            if li:
                parts = list(li.stripped_strings)
                if len(parts) >= 2:
                    name, title = parts[0], parts[1]
                elif len(parts) == 1:
                    name = parts[0]

        fields = {
            "website url": website_url['href'] if website_url else "",
            "notes": notes.text.strip() if notes else "",
            "location": location.text.strip() + ", US" if location else "Austin, US",
            "industry": "Accounting",
            "years": years.strip() if years else "",
            "Decision Maker Name": name,
            "Decision Maker Title": title
        }

        print(f"ğŸ“¦ Extracted fields: {fields}")
        airtable.insert(fields)
        print(f"âœ… Uploaded to Airtable: {fields['website url']}")

    except Exception as e:
        print(f"âŒ Failed scraping {url}: {e}")


def main():
    print("ğŸ”„ Starting full BBB scrape...")
    links = get_profile_links()

    for url in links:
        scrape_profile(url)
        time.sleep(WAIT_BETWEEN)

    print("ğŸ Done.")


if __name__ == "__main__":
    main()
